{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joanne2363/Joanne/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oNThAuHkZ7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorboardX\n",
        "# !pip install efficientnet_pytorch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKI5vHIViJIj",
        "colab_type": "code",
        "outputId": "8f07567d-9672-4e5b-9021-b21f5d6d869e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzNWnS2d5kj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1QJ8XyQ50hN",
        "colab_type": "code",
        "outputId": "fdec1814-fc3e-4610-e66d-d0535b440e3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile models/__init__.py\n",
        "from .AlexNet import AlexNet"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing models/__init__.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyId6bnj6AY6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('models')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLUv7JPYh4jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# augmentation = drive.CreateFile({'id':'1qw9CVmePKFtWn5AFoJjalULuTlzTbibO'})\n",
        "# augmentation.GetContentFile('augmentation.py')\n",
        "# addataset = drive.CreateFile({'id':'15An-7_wQ1a3QZ41UapjlVewZXTRjl5vt'})\n",
        "# addataset.GetContentFile('addataset.py')\n",
        "# admodels = drive.CreateFile({'id':'1dINUuA5HXRP8KxR0FReycZh0mpApNLZS'})\n",
        "# admodels.GetContentFile('admodels.py')\n",
        "utils = drive.CreateFile({'id':'1r624iAVyqp_ExFnls7ZgaGjeGzabbYPL'})\n",
        "utils.GetContentFile('utils.py')\n",
        "config = drive.CreateFile({'id':'1chG7-jTIDkgaxzyDCCGFdkuAcjYeXojV'})\n",
        "config.GetContentFile('config.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5cXn2pMXotr",
        "colab_type": "code",
        "outputId": "69d1e0ba-ca60-4c6a-f6f5-d5a81e982d29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from config import opt\n",
        "# import torchvision.models as M\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "import torch.nn as nn\n",
        "from tensorboardX import SummaryWriter\n",
        "%load_ext tensorboard\n",
        "import datetime\n",
        "import os\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import utils\n",
        "# from utils import RunningMean\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import random\n",
        "# import models\n",
        "import time\n",
        "from utils import AverageMeter\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwPsLzXQ8wlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(**kwargs):\n",
        "  def getmodel(arch,pretrained,num_class):\n",
        "    # print('[+] loading model...', end = '',flush = True)\n",
        "    # model = M.resnet50(pretrained=True)\n",
        "    # model.fc = torch.nn.Linear(2048, 9)\n",
        "    # print(\"=> using pre-trained model '{}'\".format(model))\n",
        "    # model.cuda()\n",
        "    # print('Done')\n",
        "    # return model\n",
        "    if arch == 'efficientnet': \n",
        "      model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "      model._fc.out_features = num_class\n",
        "\n",
        "    elif arch == 'resnet101':\n",
        "      model = models.resnet101(pretrained = pretrained)\n",
        "      num_ftrs = model.fc.in_features\n",
        "      model.fc = torch.nn.Linear(num_ftrs, num_class)\n",
        "\n",
        "    elif arch == 'alexnet':\n",
        "      model = models.alexnet(pretrained = pretrained)\n",
        "      num_ftrs = model.classifier[6].in_features\n",
        "      model.classifier[6] = nn.Linear(num_ftrs, num_class)\n",
        "\n",
        "    elif arch == 'vgg16':\n",
        "      model = models.vgg16(pretrained = pretrained)\n",
        "      num_ftrs = model.classifier[6].in_features\n",
        "      model.classifier[6] = torch.nn.Linear(num_ftrs,num_class)\n",
        "\n",
        "    elif arch == 'densenet121':\n",
        "      model = models.densenet121(pretrained = pretrained)\n",
        "      num_ftrs = model.classifier.in_features\n",
        "      model.classifier = torch.nn.Linear(num_ftrs, num_class)\n",
        "    \n",
        "    elif arch == 'googlenet':\n",
        "      model = models.googlenet(pretrained = pretrained)\n",
        "      num_ftrs = model.fc.in_features\n",
        "      model.fc = torch.nn.Linear(num_ftrs, num_class)\n",
        "    \n",
        "    elif arch == 'inception_v3':\n",
        "      model = models.inception_v3(pretrained = pretrained)\n",
        "      num_ftrs = model.AuxLogits.fc.in_features\n",
        "      model.AuxLogits.fc = torch.nn.Linear(num_ftrs, num_class)\n",
        "      num_ftrs = model.fc.in_features\n",
        "      model.fc = torch.nn.Linear(num_ftrs, num_class)\n",
        "    \n",
        "    print(\"=> using pre-trained model '{}'\".format(model))\n",
        "    return model\n",
        "\n",
        "\n",
        "  def train(train_loader, model, criterion, optimizer, epoch, arch):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    acc = AverageMeter()\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    # 從訓練集迭代器中獲取訓練數據\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "      # 評估圖片讀取耗時\n",
        "      data_time.update(time.time() - end)\n",
        "      # 將圖片標籤轉化為tensor\n",
        "      # image_var = torch.tensor(images).cuda(async=True)\n",
        "      # label = torch.tensor(target).cuda(async=True)\n",
        "      image_var = torch.tensor(images).cuda()\n",
        "      label = torch.tensor(target).cuda()\n",
        "\n",
        "      # 將圖片輸入網絡，前傳，生成預測值\n",
        "      y_pred = model(image_var)\n",
        "\n",
        "      # 計算 loss\n",
        "      if arch == 'inception_v3':\n",
        "        loss = criterion(y_pred[0], label)\n",
        "      else:\n",
        "        loss = criterion(y_pred, label)\n",
        "      losses.update(loss.item(), images.size(0))\n",
        "\n",
        "      # 計算 top1 正確率\n",
        "      if arch == 'inception_v3':\n",
        "        prec, PRED_COUNT = utils.accuracy(y_pred[0].data, target, topk=(1, 1))\n",
        "      else:\n",
        "        prec, PRED_COUNT = utils.accuracy(y_pred.data, target, topk=(1, 1))\n",
        "      acc.update(prec, PRED_COUNT)\n",
        "\n",
        "      # 對梯度進行反向傳播，使用隨機梯度下降更新網絡權重\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      \n",
        "      # 評估訓練耗時\n",
        "      batch_time.update(time.time() - end)\n",
        "      end = time.time()\n",
        "\n",
        "      # 打印耗时与结果\n",
        "      if i % print_freq == 0:\n",
        "        print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "              'Train Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "              'Trans Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "              'Accuray {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
        "               epoch, i, len(train_loader), batch_time = batch_time, \n",
        "               data_time = data_time, loss = losses, acc = acc)\n",
        "              )\n",
        "      \n",
        "      # 創建 train 日誌文件\n",
        "      if not os.path.exists(txtlog_path_pre):\n",
        "        os.makedirs(txtlog_path_pre)\n",
        "        with open(traintxtlog_path, 'w') as acc_file:\n",
        "          pass\n",
        "      # 在日志文件中紀錄每個epoch的精度和loss\n",
        "      with open(traintxtlog_path, 'a') as acc_file:\n",
        "        # acc_file.write('Epoch: [{0}][{1}/{2}]\\t'\n",
        "        #                'Train Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "        #                'Trans Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "        #                'Loss {loss.val:.3f} ({loss.avg:.3f})\\t'\n",
        "        #                'Accuray {acc.val:.3f} ({acc.avg:.3f})\\t \\n'.format(\n",
        "        #                 epoch, i, len(train_loader), batch_time = batch_time,\n",
        "        #                 data_time = data_time, loss = losses, acc = acc)\n",
        "        #               )\n",
        "        acc_file.write('Epoch:[%2d][%2d/%2d], Train Time(s):%.3f(,%.3f), Trans Time(s):%.3f(,%.3f), Loss:%.3f(,%.3f), Accuray:%.3f(,%.3f) \\n '\n",
        "                       % (epoch, i, len(train_loader),batch_time.val, batch_time.avg,\n",
        "                          data_time.val, data_time.avg,\n",
        "                          losses.val, losses.avg,acc.val, acc.avg)\n",
        "                      )\n",
        "    return acc.avg, losses.avg\n",
        "\n",
        "\n",
        "  # 驗證函數\n",
        "  def validate(val_loader, model, criterion, arch):\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    acc = AverageMeter()\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (images, labels) in enumerate(val_loader):\n",
        "      # image_var = torch.tensor(images).cuda(async=True)\n",
        "      # target = torch.tensor(labels).cuda(async=True)\n",
        "      image_var = torch.tensor(images).cuda()\n",
        "      target = torch.tensor(labels).cuda()\n",
        "\n",
        "      # 圖片前傳。驗證和測試時不需要更新網絡權重，所以使用 torch.no_grad()，表示不計算權重\n",
        "      with torch.no_grad():\n",
        "        y_pred = model(image_var)\n",
        "        loss = criterion(y_pred, target)\n",
        "      \n",
        "      # measure accuracy\n",
        "      prec, PRED_COUNT = utils.accuracy(y_pred.data, labels, topk=(1, 1))\n",
        "      losses.update(loss.item(), images.size(0))\n",
        "      acc.update(prec, PRED_COUNT)\n",
        "\n",
        "      # measure training time\n",
        "      batch_time.update(time.time() - end)\n",
        "      end = time.time()\n",
        "\n",
        "      if i % print_freq == 0:\n",
        "        print('TrainVal: [{0}/{1}]\\t'\n",
        "              'Train Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "              'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "              'Accuray {acc.val:.3f} ({acc.avg:.3f})'.format(\n",
        "            i, len(val_loader), batch_time=batch_time, loss=losses, acc=acc))\n",
        "\n",
        "    print(' * Accuray {acc.avg:.3f}'.format(acc=acc), '(Previous Best Acc: %.3f)' % best_precision,\n",
        "          ' * Loss {loss.avg:.3f}'.format(loss=losses), 'Previous Lowest Loss: %.3f)' % lowest_loss)\n",
        "    return acc.avg, losses.avg\n",
        "\n",
        "\n",
        "  # 程序主体\n",
        "  torch.cuda.empty_cache()\n",
        "  opt.parse(kwargs)\n",
        "  traindir = opt.train_data_root\n",
        "  valdir = opt.val_data_root\n",
        "  print('=========================================')\n",
        "  date = str(datetime.date.today())\n",
        "  basic_path = '/content/drive/My Drive/Colab Notebooks/aiad_2/'\n",
        "  best_path_pre   = basic_path + 'checkpoints/' + date + '/' + opt.arch + '/'\n",
        "  checkpoint_path = best_path_pre + str(opt.num_class) + '_' + date + '_checkpoint.pth'\n",
        "  best_loss_path  = best_path_pre + str(opt.num_class) + '_' + date + '_loss_best.pth'\n",
        "  best_acc_path   = best_path_pre + str(opt.num_class) + '_' + date + '_acc_best.pth'\n",
        "\n",
        "  txtlog_path_pre  = basic_path + 'logs/' + date + '/' + opt.arch + '/'\n",
        "  txtlog_path      = txtlog_path_pre + str(opt.num_class) + '_' + 'txtlog.txt'\n",
        "  traintxtlog_path = txtlog_path_pre + str(opt.num_class) + '_' + 'traintxtlog.txt'\n",
        "  besttxtlog_path  = txtlog_path_pre + str(opt.num_class) + '_' + 'besttxtlog.txt'\n",
        "  tensorBoard_path = txtlog_path_pre + 'tensorBoardX/' + str(opt.num_class) + '/'\n",
        "\n",
        "  # print('traindir:',traindir)\n",
        "  # print('valdir:',valdir)\n",
        "  # print('best_loss_path:', best_loss_path)\n",
        "  # print('best_path_pre:', best_path_pre)\n",
        "  # print('tensorBoard_path:', tensorBoard_path)\n",
        "  # print('arch:', opt.arch)\n",
        "  # print('pretrained:', opt.pretrained)\n",
        "  # print('num_class:', opt.num_class)\n",
        "  if not os.path.exists(tensorBoard_path):\n",
        "        os.makedirs(tensorBoard_path)\n",
        "  writer = SummaryWriter(tensorBoard_path)\n",
        "\n",
        "  lr = 1e-1\n",
        "  best_precision = 0\n",
        "  lowest_loss = 100\n",
        "\n",
        "  # 設定打印頻率，即多少step打印一次，用於觀察loss和acc的實時變化\n",
        "  # 打印結果中，括號前面為實時loss和acc,括號內為epoch內平均loos和acc\n",
        "  print_freq = 1\n",
        "  # 驗證集比例\n",
        "  # val_ratio = 0.12\n",
        "  # 是否只驗證，不訓練\n",
        "  evaluate = False\n",
        "  # 是否從斷點繼續跑\n",
        "  resume = False\n",
        "  # 忍受該指標多少個step不變化，就調整學習率\n",
        "  patience = 0\n",
        "  # 圖片歸一化，由於採用Pytorch預訓練網絡，因此這裡直接採用Pytorch網絡的參數\n",
        "  normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "  train_dataset = datasets.ImageFolder(\n",
        "                    opt.train_data_root,\n",
        "                    transforms.Compose([\n",
        "                        # transforms.RandomResizedCrop(IMAGE_SIZE),\n",
        "                        transforms.Resize(opt.input_size + 32),\n",
        "                        transforms.CenterCrop(opt.input_size),\n",
        "                        transforms.RandomHorizontalFlip(),\n",
        "                        transforms.ToTensor(),\n",
        "                        normalize,\n",
        "                    ])\n",
        "                  )\n",
        "  val_dataset = datasets.ImageFolder(\n",
        "                  opt.val_data_root,\n",
        "                  transforms.Compose([\n",
        "                      transforms.Resize(opt.input_size + 32),\n",
        "                      transforms.CenterCrop(opt.input_size),\n",
        "                      transforms.ToTensor(),\n",
        "                      normalize,\n",
        "                  ])\n",
        "                )\n",
        " # 生成圖片迭代器\n",
        "  train_dataLoader = DataLoader(dataset = train_dataset, batch_size = opt.batch_size, \n",
        "                                shuffle = True, pin_memory = True, num_workers = opt.num_workers )\n",
        "  val_dataLoader = DataLoader(dataset = val_dataset, batch_size = opt.batch_size, \n",
        "                              shuffle = False, pin_memory = False, num_workers = 1)\n",
        "  # 創建模型\n",
        "  model = getmodel(opt.arch,opt.pretrained,opt.num_class)\n",
        "  if opt.use_gpu: model.cuda()\n",
        "\n",
        "  # 使用交叉熵損失函數\n",
        "  criterion = nn.CrossEntropyLoss().cuda()\n",
        "  \n",
        "  # 優化器，使用帶amsgrad的Adam\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay = opt.weight_decay, amsgrad = True)\n",
        "  \n",
        "  if evaluate:\n",
        "    validate(val_dataLoader, model, criterion)\n",
        "  else:\n",
        "    # 開始訓練\n",
        "    for epoch in range(opt.max_epoch):\n",
        "      if patience == 3:\n",
        "        patience = 0\n",
        "        model.load_state_dict(torch.load(best_loss_path)['state_dict'])\n",
        "        lr = lr / 10\n",
        "        print('loss has increased lr divide 10 lr now is :%e'%(lr))\n",
        "        # 優化器，使用帶amsgrad的Adam\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay = opt.weight_decay, amsgrad = True)\n",
        "\n",
        "      # train for one epoch\n",
        "      precisiont, avg_losst = train(train_dataLoader, model, criterion, optimizer, epoch, opt.arch)\n",
        "      # train(train_dataLoader, model, criterion, optimizer, epoch, opt.arch)\n",
        "      writer.add_scalar('Train/Acc', precisiont, epoch + 1)\n",
        "      writer.add_scalar('Train/Loss', avg_losst, epoch + 1)\n",
        "      # evaluate on validation set\n",
        "      precision, avg_loss = validate(val_dataLoader, model, criterion, opt.arch)\n",
        "      writer.add_scalar('Val/Acc', precision, epoch + 1)\n",
        "      writer.add_scalar('Val/Loss', avg_loss, epoch + 1)\n",
        "\n",
        "      # print('epoch:',epoch + 1)\n",
        "      # print('precisiont:',precisiont)\n",
        "      # print('avg_losst:',avg_losst)\n",
        "      # print('precision:',precision)\n",
        "      # print('avg_loss:',avg_loss)\n",
        "\n",
        "      # 創建日誌文件\n",
        "      if not os.path.exists(best_path_pre):\n",
        "        os.makedirs(best_path_pre)\n",
        "        with open(txtlog_path, 'w') as acc_file:\n",
        "          pass\n",
        "      # 在日志文件中紀錄每個epoch的精度和loss\n",
        "      with open(txtlog_path, 'a') as acc_file:\n",
        "        acc_file.write('Epoch: %2d, Val Precision: %.4f, Val Loss: %.4f, DateTime: %s \\n' \n",
        "                       % (epoch, precision, avg_loss, \n",
        "                          time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time()))\n",
        "                         )\n",
        "                      )\n",
        " \n",
        "      # 紀錄最高精度與最低loss，保存最新模型與最佳模型\n",
        "      is_best = precision > best_precision\n",
        "      is_lowest_loss = avg_loss < lowest_loss\n",
        "      best_precision = max(precision, best_precision)\n",
        "      lowest_loss = min(avg_loss, lowest_loss)\n",
        "      state = {\n",
        "          'epoch': epoch,\n",
        "          'state_dict': model.state_dict(),\n",
        "          'best_precision': best_precision,\n",
        "          'lowest_loss': lowest_loss,\n",
        "          # 'stage': stage,\n",
        "          'lr': lr,\n",
        "      }\n",
        "      utils.save_checkpoint(state, is_best, is_lowest_loss, checkpoint_path, best_acc_path, best_loss_path)\n",
        "      \n",
        "      if not is_best:\n",
        "        patience += 1\n",
        "      # # 判斷是否進行下一個stage\n",
        "      # if (epoch + 1) in np.cumsum(stage_epochs)[:-1]:\n",
        "      #   stage += 1\n",
        "      #   optimizer = adjust_learning_rate()\n",
        "      #   model.load_state_dict(torch.load(BEST_ACC_PATH)['state_dict'])\n",
        "      #   print('Step into next stage')\n",
        "      #   with open(TXTLOG_PATH, 'a') as acc_file:\n",
        "      #     acc_file.write('---------------Step into next stage----------------\\n')\n",
        "  \n",
        "  # 紀錄最佳分數\n",
        "  if not os.path.exists(best_path_pre):\n",
        "    os.makedirs(best_path_pre)\n",
        "    with open(txtlog_path, 'w') as acc_file:\n",
        "      pass\n",
        "  with open(txtlog_path, 'a') as acc_file:\n",
        "    acc_file.write('%s  * best acc: %.8f  \\n' \n",
        "                    % (time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time())),best_precision)\n",
        "                  )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udJZzKCdPyYd",
        "colab_type": "code",
        "outputId": "9b84bf7d-bff9-4c43-e949-2c6979d1dbd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main(max_epoch = 60, arch = 'googlenet', num_class = 16, evaluate = False, batch_size = 64, input_size = 299,\n",
        "     train_data_root = '/content/drive/My Drive/Colab Notebooks/aiad_2/data/train_16/',\n",
        "     val_data_root = '/content/drive/My Drive/Colab Notebooks/aiad_2/data/val_16/' )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "user config:\n",
            "env default\n",
            "arch googlenet\n",
            "pretrained True\n",
            "num_class 16\n",
            "input_size 299\n",
            "evaluate False\n",
            "train_data_root /content/drive/My Drive/Colab Notebooks/aiad_2/data/train_16/\n",
            "val_data_root /content/drive/My Drive/Colab Notebooks/aiad_2/data/val_16/\n",
            "load_model_path /content/drive/My Drive/Colab Notebooks/aiad_2/checkpoints/model.pth\n",
            "batch_size 64\n",
            "use_gpu True\n",
            "num_workers 4\n",
            "print_freq 20\n",
            "debug_file /tmp/debug\n",
            "result_file result.csv\n",
            "max_epoch 60\n",
            "lr 0.1\n",
            "lr_decay 0.95\n",
            "weight_decay 0.0001\n",
            "img_size 224\n",
            "parse <bound method parse of <config.DefaultConfig object at 0x7f4c56fdda90>>\n",
            "=========================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/checkpoints/googlenet-1378be20.pth\n",
            "100%|██████████| 49.7M/49.7M [00:02<00:00, 19.4MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "=> using pre-trained model 'GoogLeNet(\n",
            "  (conv1): BasicConv2d(\n",
            "    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "  (conv2): BasicConv2d(\n",
            "    (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (conv3): BasicConv2d(\n",
            "    (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "  (inception3a): Inception(\n",
            "    (branch1): BasicConv2d(\n",
            "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (inception3b): Inception(\n",
            "    (branch1): BasicConv2d(\n",
            "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (maxpool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "  (inception4a): Inception(\n",
            "    (branch1): BasicConv2d(\n",
            "      (conv): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(208, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (inception4b): Inception(\n",
            "    (branch1): BasicConv2d(\n",
            "      (conv): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (inception4c): Inception(\n",
            "    (branch1): BasicConv2d(\n",
            "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (inception4d): Inception(\n",
            "    (branch1): BasicConv2d(\n",
            "      (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (inception4e): Inception(\n",
            "    (branch1): BasicConv2d(\n",
            "      (conv): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
            "  (inception5a): Inception(\n",
            "    (branch1): BasicConv2d(\n",
            "      (conv): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (inception5b): Inception(\n",
            "    (branch1): BasicConv2d(\n",
            "      (conv): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (branch2): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch3): Sequential(\n",
            "      (0): BasicConv2d(\n",
            "        (conv): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (branch4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
            "      (1): BasicConv2d(\n",
            "        (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=1024, out_features=16, bias=True)\n",
            ")'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-1fbad847bf07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m main(max_epoch = 60, arch = 'googlenet', num_class = 16, evaluate = False, batch_size = 64, input_size = 299,\n\u001b[1;32m      2\u001b[0m      \u001b[0mtrain_data_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/Colab Notebooks/aiad_2/data/train_16/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m      val_data_root = '/content/drive/My Drive/Colab Notebooks/aiad_2/data/val_16/' )\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-399abdda2136>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m       \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m       \u001b[0mprecisiont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_losst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m       \u001b[0;31m# train(train_dataLoader, model, criterion, optimizer, epoch, opt.arch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train/Acc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecisiont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-399abdda2136>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, epoch, arch)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# 從訓練集迭代器中獲取訓練數據\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m       \u001b[0;31m# 評估圖片讀取耗時\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0mdata_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iVYZ1oZU3-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main(max_epoch = 40,arch = 'alexnet', num_class = 16, evaluate = False, batch_size = 64,\n",
        "     train_data_root = '/content/drive/My Drive/Colab Notebooks/aiad_2/data/train_16/',\n",
        "     val_data_root = '/content/drive/My Drive/Colab Notebooks/aiad_2/data/val_16/' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55U4WVL6JU0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/content/drive/My Drive/Colab Notebooks/aiad_2/logs/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR4ydtg9FIJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.models as models\n",
        "import torch\n",
        "model = models.inception_v3(pretrained = True)\n",
        "print(\"=> using pre-trained model '{}'\".format(model))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}